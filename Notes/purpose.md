

# Why build this?

Transformers are about to predict next tokens with incredible accuracy.
The reason they are able to do this, is because of the incredible density of feedback that is given to a single combined model. Each word in a seqeunce is predicted by the model and provides error correction.

I especially think that this would be useful for analyising causal links between medical data.

